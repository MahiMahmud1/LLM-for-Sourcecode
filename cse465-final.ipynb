{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:01:35.611098Z","iopub.execute_input":"2025-08-20T08:01:35.611686Z","iopub.status.idle":"2025-08-20T08:01:35.871016Z","shell.execute_reply.started":"2025-08-20T08:01:35.611659Z","shell.execute_reply":"2025-08-20T08:01:35.870211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -q \"transformers>=4.42\" \"accelerate>=0.33\" bitsandbytes sentencepiece\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:01:35.872425Z","iopub.execute_input":"2025-08-20T08:01:35.873106Z","iopub.status.idle":"2025-08-20T08:02:48.986008Z","shell.execute_reply.started":"2025-08-20T08:01:35.873086Z","shell.execute_reply":"2025-08-20T08:02:48.985112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re, json, textwrap, tempfile, subprocess, sys, time, traceback\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Pick an open-source code model (fits on Kaggle T4 in 4-bit)\nMODEL_ID = os.environ.get(\"MODEL_ID\", \"Qwen/Qwen2.5-Coder-7B-Instruct\")\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Loading modelâ€¦ (this can take a couple of minutes the first time)\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    device_map=\"auto\",        # accelerate places shards/devices for us\n    torch_dtype=torch.float16,\n    load_in_4bit=True,        # 4-bit quantization -> fits T4\n)\n\n# ðŸš« Do NOT pass `device=` here since we used device_map=\"auto\"\ngen = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\nprint(\"Model loaded:\", MODEL_ID)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:02:48.986951Z","iopub.execute_input":"2025-08-20T08:02:48.987176Z","iopub.status.idle":"2025-08-20T08:06:09.742599Z","shell.execute_reply.started":"2025-08-20T08:02:48.987154Z","shell.execute_reply":"2025-08-20T08:06:09.741999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simple chat helper for instruction-tuned models\ndef chat(messages, max_new_tokens=1024, temperature=0.2, top_p=0.95):\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    out = gen(\n        prompt,\n        do_sample=temperature > 0,\n        temperature=temperature,\n        top_p=top_p,\n        max_new_tokens=max_new_tokens,\n        eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|im_end|>\")],\n    )[0][\"generated_text\"]\n    return out[len(prompt):].strip()\n\n# Extract ```python ... ``` blocks (or plain ``` ... ```)\nimport re\nCODE_FENCE = re.compile(r\"```(?:python|py|Python)?\\s*(.*?)```\", re.DOTALL)\ndef extract_code_blocks(text: str):\n    blocks = CODE_FENCE.findall(text)\n    return [b.strip() for b in blocks] if blocks else []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:06:28.429080Z","iopub.execute_input":"2025-08-20T08:06:28.429544Z","iopub.status.idle":"2025-08-20T08:06:28.434890Z","shell.execute_reply.started":"2025-08-20T08:06:28.429520Z","shell.execute_reply":"2025-08-20T08:06:28.434249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Safer construction: avoid raw ``` inside f-strings\nFENCE = \"```\"\n\ndef prompt_planning(problem, language=\"Python\"):\n    return (\n        f\"You are a programmer tasked with generating a **plan** to solve the problem using {language}.\\n\"\n        \"Do NOT write code.\\n\\n\"\n        \"## Problem\\n\"\n        f\"{problem}\\n\\n\"\n        \"### Expected Output format\\n\"\n        \"### Problem Understanding\\n\"\n        \"- Briefly restate the task and constraints.\\n\\n\"\n        \"### Algorithm to solve the original problem\\n\"\n        \"- Name the approach and why it fits.\\n\"\n        \"- Important edge cases.\\n\\n\"\n        \"### Plan\\n\"\n        \"- A numbered, step-by-step plan that is detailed but code-free.\\n\"\n    ).strip()\n\n\ndef prompt_simulate_plan(problem, plan, language=\"Python\"):\n    return (\n        \"You are verifying whether the plan will produce the correct output.\\n\\n\"\n        \"## Problem\\n\"\n        f\"{problem}\\n\\n\"\n        \"### Plan\\n\"\n        f\"{plan}\\n\\n\"\n        \"### Expected Output format\\n\"\n        \"### Simulation\\n\"\n        \"- Choose a simple sample input and apply the plan step-by-step to produce output.\\n\"\n        \"- Compare with the expected result implied by the problem.\\n\\n\"\n        \"### Plan Evaluation\\n\"\n        \"- Print exactly one line: either **No Need to Modify Plan** OR **Plan Modification Needed** (and say why).\\n\"\n    ).strip()\n\n\ndef prompt_refine_plan(problem, plan, sim_report, language=\"Python\"):\n    return (\n        \"You have a plan that needs improvements based on the simulation report.\\n\\n\"\n        \"## Problem\\n\"\n        f\"{problem}\\n\\n\"\n        \"### Current Plan\\n\"\n        f\"{plan}\\n\\n\"\n        \"### Simulation Report\\n\"\n        f\"{sim_report}\\n\\n\"\n        \"### Expected Output format\\n\"\n        \"### New Plan\\n\"\n        \"- Provide ONLY a corrected step-by-step plan. No explanations, no code.\\n\"\n    ).strip()\n\n\ndef prompt_code(problem, plan, language=\"Python\"):\n    # Avoid literal ``` in the f-string by inserting from FENCE\n    return (\n        f\"You are now writing {language} code to implement the plan. Output ONLY a single code block.\\n\\n\"\n        \"## Problem\\n\"\n        f\"{problem}\\n\\n\"\n        \"### Plan\\n\"\n        f\"{plan}\\n\\n\"\n        \"### Important instructions\\n\"\n        f\"- Output must be a single code block fenced with {FENCE}.\\n\"\n        \"- No extra commentary.\\n\"\n    ).strip()\n\n\ndef prompt_debug(problem, plan, buggy_code, test_log, language=\"Python\"):\n    # Build the code section with fences safely\n    buggy_block = FENCE + language + \"\\n\" + buggy_code + \"\\n\" + FENCE\n    return (\n        f\"You received {language} code that fails some tests. Fix it.\\n\\n\"\n        \"## Problem\\n\"\n        f\"{problem}\\n\\n\"\n        \"### Plan\\n\"\n        f\"{plan}\\n\\n\"\n        \"### Buggy Code\\n\"\n        f\"{buggy_block}\\n\\n\"\n        \"### Test Report\\n\"\n        f\"{test_log}\\n\\n\"\n        \"### Expected Output format\\n\"\n        \"### Simulation with failed test case\\n\"\n        \"- Choose one failing case; simulate step-by-step to locate the issue.\\n\\n\"\n        \"### Debugging Notes\\n\"\n        \"- Is the plan wrong, or is the code deviating from the plan? Explain briefly.\\n\"\n        \"- Describe the fix.\\n\\n\"\n        \"### Modified Code\\n\"\n        f\"- Output ONLY a single corrected {language} code block fenced with {FENCE}.\\n\"\n        \"- Do NOT include test code.\\n\"\n    ).strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:06:32.677558Z","iopub.execute_input":"2025-08-20T08:06:32.678186Z","iopub.status.idle":"2025-08-20T08:06:32.685571Z","shell.execute_reply.started":"2025-08-20T08:06:32.678163Z","shell.execute_reply":"2025-08-20T08:06:32.684678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tempfile, subprocess, sys, json, os\n\ndef run_user_code_and_check(py_code: str, tests_code: str, time_limit=3.0):\n    \"\"\"\n    py_code: candidate solution (string).\n    tests_code: Python code that defines test functions t1(...), t2(...), ...\n                must end with TEST_FUNCS = [t1, t2, ...]\n    Returns: (passed: bool, log: str)\n    \"\"\"\n    with tempfile.TemporaryDirectory() as td:\n        sol_path = os.path.join(td, \"solution.py\")\n        with open(sol_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(py_code)\n\n        harness_path = os.path.join(td, \"harness.py\")\n        with open(harness_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(\n                \"import importlib.util, json, sys\\n\"\n                \"solution_path = sys.argv[1]\\n\"\n                \"spec = importlib.util.spec_from_file_location('solution', solution_path)\\n\"\n                \"mod = importlib.util.module_from_spec(spec)\\n\"\n                \"spec.loader.exec_module(mod)\\n\"\n            )\n            f.write(\"\\n# --- BEGIN TESTS ---\\n\")\n            f.write(tests_code)\n            f.write(\"\\n# --- END TESTS ---\\n\")\n            f.write(\n                \"results = []\\n\"\n                \"for fn in TEST_FUNCS:\\n\"\n                \"    try:\\n\"\n                \"        ok, msg = fn(mod)\\n\"\n                \"    except Exception as e:\\n\"\n                \"        ok, msg = False, f'exception: {e}'\\n\"\n                \"    results.append((bool(ok), str(msg)))\\n\"\n                \"print(json.dumps(results))\\n\"\n            )\n\n        try:\n            proc = subprocess.run(\n                [sys.executable, harness_path, sol_path],\n                capture_output=True, text=True, timeout=time_limit\n            )\n        except subprocess.TimeoutExpired:\n            return False, \"timeout\"\n\n        if proc.returncode != 0:\n            return False, f\"harness error: {proc.stderr.strip()}\"\n\n        out = proc.stdout.strip()\n        try:\n            results = json.loads(out)\n        except Exception:\n            return False, f\"bad harness output: {out[:500]}\"\n\n        passed = all(p for p, _ in results)\n        return passed, str(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:06:44.038989Z","iopub.execute_input":"2025-08-20T08:06:44.039244Z","iopub.status.idle":"2025-08-20T08:06:44.047051Z","shell.execute_reply.started":"2025-08-20T08:06:44.039228Z","shell.execute_reply":"2025-08-20T08:06:44.046376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_problem = \"\"\"\nWrite a function `generate_integers(a, b)` that returns the even integers between a and b inclusive,\nin ascending order, regardless of whether a <= b or a > b.\n\"\"\"\n\n# Correct tests: (10,14) should yield [10,12,14]\ntests_code = \"\"\"\ndef t1(mod):\n    try:\n        return (mod.generate_integers(2,8) == [2,4,6,8], 't1')\n    except Exception as e:\n        return (False, f't1 exception {e}')\n\ndef t2(mod):\n    try:\n        return (mod.generate_integers(8,2) == [2,4,6,8], 't2')\n    except Exception as e:\n        return (False, f't2 exception {e}')\n\ndef t3(mod):\n    try:\n        return (mod.generate_integers(10,14) == [10,12,14], 't3')\n    except Exception as e:\n        return (False, f't3 exception {e}')\n\nTEST_FUNCS = [t1, t2, t3]\n\"\"\".strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:06:48.884936Z","iopub.execute_input":"2025-08-20T08:06:48.885208Z","iopub.status.idle":"2025-08-20T08:06:48.889162Z","shell.execute_reply.started":"2025-08-20T08:06:48.885188Z","shell.execute_reply":"2025-08-20T08:06:48.888408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def codesim(problem, tests_code, language=\"Python\", p=3, d=3,\n            model_temperature=0.2, max_new_tokens=900):\n    # 1) Planning\n    plan = chat(\n        [\n            {\"role\":\"system\",\"content\":\"You are a careful software engineer who follows instructions exactly.\"},\n            {\"role\":\"user\",\"content\": prompt_planning(problem, language)}\n        ],\n        max_new_tokens=700, temperature=model_temperature\n    )\n    sim_report = chat(\n        [\n            {\"role\":\"system\",\"content\":\"You are a careful software engineer who follows instructions exactly.\"},\n            {\"role\":\"user\",\"content\": prompt_simulate_plan(problem, plan, language)}\n        ],\n        max_new_tokens=600, temperature=model_temperature\n    )\n\n    if \"Plan Modification Needed\" in sim_report:\n        plan = chat(\n            [\n                {\"role\":\"system\",\"content\":\"You are a careful software engineer who follows instructions exactly.\"},\n                {\"role\":\"user\",\"content\": prompt_refine_plan(problem, plan, sim_report, language)}\n            ],\n            max_new_tokens=600, temperature=model_temperature\n        )\n\n    # 2) Coding\n    codegen_resp = chat(\n        [\n            {\"role\":\"system\",\"content\":\"You are a careful software engineer who follows instructions exactly.\"},\n            {\"role\":\"user\",\"content\": prompt_code(problem, plan, language)}\n        ],\n        max_new_tokens=max_new_tokens, temperature=model_temperature\n    )\n    blocks = extract_code_blocks(codegen_resp)\n    code = blocks[0] if blocks else codegen_resp\n\n    passed, log = run_user_code_and_check(code, tests_code)\n    if passed:\n        return {\"stage\":\"codegen_passed\", \"plan\":plan, \"log\":log, \"code\":code}\n\n    # 3) Debugging attempts\n    last_code = code\n    for _ in range(d):\n        dbg_resp = chat(\n            [\n                {\"role\":\"system\",\"content\":\"You are a careful software engineer who follows instructions exactly.\"},\n                {\"role\":\"user\",\"content\": prompt_debug(problem, plan, last_code, log, language)}\n            ],\n            max_new_tokens=1000, temperature=model_temperature\n        )\n        blocks = extract_code_blocks(dbg_resp)\n        if blocks:\n            last_code = blocks[0]\n        passed, log = run_user_code_and_check(last_code, tests_code)\n        if passed:\n            return {\"stage\":\"debug_passed\", \"plan\":plan, \"log\":log, \"code\":last_code}\n\n    return {\"stage\":\"failed\", \"plan\":plan, \"log\":log, \"code\":last_code}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:06:53.173210Z","iopub.execute_input":"2025-08-20T08:06:53.173754Z","iopub.status.idle":"2025-08-20T08:06:53.181563Z","shell.execute_reply.started":"2025-08-20T08:06:53.173716Z","shell.execute_reply":"2025-08-20T08:06:53.180888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\n\nresult = codesim(example_problem, tests_code, p=3, d=3)\nprint(\"Stage:\", result[\"stage\"])\n\nprint(\"\\n--- PLAN (first 900 chars) ---\\n\", result[\"plan\"][:900])\n\nprint(\"\\n--- TEST RESULTS ---\")\ntry:\n    parsed = ast.literal_eval(result[\"log\"])  # parse the Python repr string\n    for i, (ok, label) in enumerate(parsed, 1):\n        mark = \"âœ“\" if ok else \"âœ—\"\n        print(f\"  {mark} {label}\")\nexcept Exception:\n    print(result[\"log\"])\n\nprint(\"\\n--- CODE ---\\n\", result[\"code\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:06:57.220362Z","iopub.execute_input":"2025-08-20T08:06:57.220638Z","iopub.status.idle":"2025-08-20T08:07:46.684070Z","shell.execute_reply.started":"2025-08-20T08:06:57.220615Z","shell.execute_reply":"2025-08-20T08:07:46.683259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# You can try a smaller/larger instruct model. Re-run Cell 2 after changing MODEL_ID.\n# Examples:\n# os.environ[\"MODEL_ID\"] = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n# os.environ[\"MODEL_ID\"] = \"Qwen/Qwen2.5-7B-Instruct\"\n# os.environ[\"MODEL_ID\"] = \"google/gemma-2-2b-it\"   # tiny, fast; not code-specialized\n# os.environ[\"MODEL_ID\"] = \"codellama/CodeLlama-7b-Instruct-hf\"  # may need license acceptance with HF account\nprint(\"Current MODEL_ID =\", os.environ.get(\"MODEL_ID\", \"Qwen/Qwen2.5-Coder-7B-Instruct\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:08:17.189279Z","iopub.execute_input":"2025-08-20T08:08:17.189563Z","iopub.status.idle":"2025-08-20T08:08:17.194154Z","shell.execute_reply.started":"2025-08-20T08:08:17.189537Z","shell.execute_reply":"2025-08-20T08:08:17.193155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tweak these to trade quality vs speed\nCODESIM_TEMPERATURE = 0.2   # lower = more deterministic\nCODESIM_P = 3               # planning tries\nCODESIM_D = 3               # debugging tries\nMAX_NEW_TOKENS = 900        # cap generation length\n\nprint({\n    \"temperature\": CODESIM_TEMPERATURE,\n    \"p\": CODESIM_P,\n    \"d\": CODESIM_D,\n    \"max_new_tokens\": MAX_NEW_TOKENS\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:08:20.391238Z","iopub.execute_input":"2025-08-20T08:08:20.391505Z","iopub.status.idle":"2025-08-20T08:08:20.396490Z","shell.execute_reply.started":"2025-08-20T08:08:20.391488Z","shell.execute_reply":"2025-08-20T08:08:20.395645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\nfrom dataclasses import dataclass\nfrom typing import List, Dict\n\n@dataclass\nclass TinyTask:\n    name: str\n    problem: str\n    tests_code: str\n\n# A tiny suite of 3 problems (add more if you like)\nTASKS: List[TinyTask] = [\n    TinyTask(\n        name=\"generate_integers\",\n        problem=(\n            \"Write a function `generate_integers(a, b)` that returns the even integers between a and b inclusive, \"\n            \"in ascending order, regardless of whether a <= b or a > b.\"\n        ),\n        tests_code=\"\"\"\ndef t1(mod):\n    try:\n        return (mod.generate_integers(2,8) == [2,4,6,8], 't1')\n    except Exception as e:\n        return (False, f't1 exception {e}')\n\ndef t2(mod):\n    try:\n        return (mod.generate_integers(8,2) == [2,4,6,8], 't2')\n    except Exception as e:\n        return (False, f't2 exception {e}')\n\ndef t3(mod):\n    try:\n        return (mod.generate_integers(10,14) == [10,12,14], 't3')\n    except Exception as e:\n        return (False, f't3 exception {e}')\n\nTEST_FUNCS = [t1, t2, t3]\n\"\"\".strip()\n    ),\n    TinyTask(\n        name=\"is_palindrome\",\n        problem=(\n            \"Write a function `is_palindrome(s)` that returns True if string s is a palindrome \"\n            \"(case-insensitive, ignore non-alphanumerics), else False.\"\n        ),\n        tests_code=\"\"\"\nimport re\ndef t1(mod):\n    try:\n        return (mod.is_palindrome(\"racecar\")==True, 't1')\n    except Exception as e:\n        return (False, f't1 exception {e}')\ndef t2(mod):\n    try:\n        return (mod.is_palindrome(\"A man, a plan, a canal: Panama\")==True, 't2')\n    except Exception as e:\n        return (False, f't2 exception {e}')\ndef t3(mod):\n    try:\n        return (mod.is_palindrome(\"hello\")==False, 't3')\n    except Exception as e:\n        return (False, f't3 exception {e}')\nTEST_FUNCS = [t1, t2, t3]\n\"\"\".strip()\n    ),\n    TinyTask(\n        name=\"nth_fib\",\n        problem=(\n            \"Write a function `nth_fib(n)` that returns the n-th Fibonacci number where nth_fib(0)=0 and nth_fib(1)=1. \"\n            \"Assume 0 <= n <= 30.\"\n        ),\n        tests_code=\"\"\"\ndef t1(mod):\n    try:\n        return (mod.nth_fib(0)==0, 't1')\n    except Exception as e:\n        return (False, f't1 exception {e}')\ndef t2(mod):\n    try:\n        return (mod.nth_fib(1)==1, 't2')\n    except Exception as e:\n        return (False, f't2 exception {e}')\ndef t3(mod):\n    try:\n        return (mod.nth_fib(10)==55, 't3')\n    except Exception as e:\n        return (False, f't3 exception {e}')\nTEST_FUNCS = [t1, t2, t3]\n\"\"\".strip()\n    ),\n]\n\ndef run_tasks(tasks: List[TinyTask]) -> List[Dict]:\n    results = []\n    for t in tasks:\n        print(f\"\\n=== Running task: {t.name} ===\")\n        r = codesim(\n            t.problem,\n            t.tests_code,\n            p=CODESIM_P,\n            d=CODESIM_D,\n            language=\"Python\",\n            model_temperature=CODESIM_TEMPERATURE,\n            max_new_tokens=MAX_NEW_TOKENS\n        )\n        # Parse test log\n        try:\n            parsed = ast.literal_eval(r[\"log\"])\n            pass_list = [bool(ok) for ok, _ in parsed]\n            labels = [lbl for _, lbl in parsed]\n        except Exception:\n            pass_list, labels = [], []\n        row = {\n            \"task\": t.name,\n            \"stage\": r[\"stage\"],\n            \"tests_passed\": int(sum(pass_list)),\n            \"tests_total\": int(len(pass_list)),\n            \"test_labels\": \",\".join(labels),\n            \"code\": r[\"code\"],\n            \"plan\": r[\"plan\"][:900],\n        }\n        print(f\" -> stage={row['stage']}   {row['tests_passed']}/{row['tests_total']} tests\")\n        results.append(row)\n    return results\n\nbatch_results = run_tasks(TASKS)\nprint(\"\\nDone. Collected\", len(batch_results), \"results.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:08:23.734000Z","iopub.execute_input":"2025-08-20T08:08:23.734686Z","iopub.status.idle":"2025-08-20T08:11:10.274751Z","shell.execute_reply.started":"2025-08-20T08:08:23.734651Z","shell.execute_reply":"2025-08-20T08:11:10.274108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame(batch_results)\ndisplay(df)\n\nout_path = \"/kaggle/working/codesim_batch_results.csv\"\ndf.to_csv(out_path, index=False)\nprint(\"Saved:\", out_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:17:21.850745Z","iopub.execute_input":"2025-08-20T08:17:21.851050Z","iopub.status.idle":"2025-08-20T08:17:21.914006Z","shell.execute_reply.started":"2025-08-20T08:17:21.851027Z","shell.execute_reply":"2025-08-20T08:17:21.913261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -q datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:17:25.711726Z","iopub.execute_input":"2025-08-20T08:17:25.712482Z","iopub.status.idle":"2025-08-20T08:17:29.930658Z","shell.execute_reply.started":"2025-08-20T08:17:25.712459Z","shell.execute_reply":"2025-08-20T08:17:29.929747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nimport json\n\ndef humaneval_tasks(n=5):\n    \"\"\"\n    Load the first n HumanEval tasks and wrap them into our TinyTask format.\n    Each item has fields: prompt (function header + docstring), test (python code),\n    and entry_point (function name).\n    We create a tests_code string that executes their 'test' code and calls check(candidate).\n    \"\"\"\n    ds = load_dataset(\"openai_humaneval\")[\"test\"]  # small dataset\n    tasks = []\n    for i in range(min(n, len(ds))):\n        item = ds[i]\n        prompt = item[\"prompt\"]           # includes def signature + docstring (no body)\n        test_src = item[\"test\"]           # Python code with a function `check(candidate)`\n        entry = item[\"entry_point\"]       # name of the function to implement\n\n        # Build a problem string from the prompt (what we show to the LLM)\n        problem = f\"\"\"\nComplete the following function in Python. Do not modify the function name or signature.\n\n{prompt}\n\"\"\".strip()\n\n        # Build tests_code that runs HumanEval's check(candidate).\n        # Use json.dumps to safely embed the test code string.\n        tests_code = f\"\"\"\nENTRY_POINT = {json.dumps(entry)}\nTEST_SRC = {json.dumps(test_src)}\n\ndef t1(mod):\n    # Executes HumanEval's test code, then calls check(candidate)\n    try:\n        ns = {{}}\n        exec(TEST_SRC, ns)\n        cand = getattr(mod, ENTRY_POINT)\n        ns['check'](cand)\n        return (True, 'human-eval')\n    except AssertionError as e:\n        return (False, f'assertion failed: {{e}}')\n    except Exception as e:\n        return (False, f'error: {{e}}')\n\nTEST_FUNCS = [t1]\n\"\"\".strip()\n\n        tasks.append(TinyTask(\n            name=f\"humaneval_{item['task_id']}\",\n            problem=problem,\n            tests_code=tests_code\n        ))\n    return tasks\n\nprint(\"HumanEval loader ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:17:34.970881Z","iopub.execute_input":"2025-08-20T08:17:34.971184Z","iopub.status.idle":"2025-08-20T08:17:36.088633Z","shell.execute_reply.started":"2025-08-20T08:17:34.971160Z","shell.execute_reply":"2025-08-20T08:17:36.087898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How many HumanEval tasks to try (keep small first; bump later)\nN = 5\n\nhe_tasks = humaneval_tasks(n=N)\nhe_results = run_tasks(he_tasks)\n\n# Compute pass@1\npassed = sum(1 for r in he_results if r[\"tests_passed\"] == r[\"tests_total\"] == 1)\npass_at_1 = passed / max(1, len(he_results))\nprint(f\"\\nHumanEval mini-run: {passed}/{len(he_results)} passed  â†’ pass@1 = {pass_at_1:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:17:42.358554Z","iopub.execute_input":"2025-08-20T08:17:42.359632Z","iopub.status.idle":"2025-08-20T08:21:36.942975Z","shell.execute_reply.started":"2025-08-20T08:17:42.359606Z","shell.execute_reply":"2025-08-20T08:21:36.942318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf_he = pd.DataFrame(he_results)\ndisplay(df_he)\n\nout_path_he = \"/kaggle/working/codesim_humaneval_results.csv\"\ndf_he.to_csv(out_path_he, index=False)\nprint(\"Saved:\", out_path_he)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T08:27:04.823248Z","iopub.execute_input":"2025-08-20T08:27:04.824036Z","iopub.status.idle":"2025-08-20T08:27:04.837586Z","shell.execute_reply.started":"2025-08-20T08:27:04.824010Z","shell.execute_reply":"2025-08-20T08:27:04.836858Z"}},"outputs":[],"execution_count":null}]}